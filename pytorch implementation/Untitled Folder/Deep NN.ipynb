{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a network with -\n",
    "2 Hidden layers and one output layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    #initialising the network to have a random linear functions. Generating a whole NN.\n",
    "    def __init__(self,D_in,H1,H2,D_out):\n",
    "        super(Net,self).__init__()\n",
    "        #now i have created a child class of Module. \n",
    "        #I can call functions of parent class \n",
    "        #and store it as objects of child class\n",
    "        #Look self.Linear as a step. That randomly initialises weights and biases for all features and training example.\n",
    "        #You can further retreive these variables by model.parameters()\n",
    "        self.Linear1=nn.Linear(D_in,H1)\n",
    "        self.Linear2=nn.Linear(H1,H2)\n",
    "        #the represents the output layer\n",
    "        self.Linear3=nn.Linear(H2,D_in)\n",
    "    def forward(self,x):\n",
    "        x=torch.sigmoid(self.Linear1(x))\n",
    "        x=torch.sigmoid(self.Linear2(x))\n",
    "        x=self.Linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remember**  \n",
    "- If you want to implement examples in one node. You can put every example in each row of a tensor torch\n",
    "- And if you want to put different features. Its always better to put them in different nodes all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this has popular training datasets\n",
    "train_dataset=dset.MNIST(root='./data',train=True,download=True,transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## setting a loss function\n",
    "criterion=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model class using Relu as a activation function\n",
    "\n",
    "class NetRelu(nn.Module):\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, D_in, H1, H2, D_out):\n",
    "        super(NetRelu, self).__init__()\n",
    "        self.linear1 = nn.Linear(D_in, H1)\n",
    "        self.linear2 = nn.Linear(H1, H2)\n",
    "        self.linear3 = nn.Linear(H2, D_out)\n",
    "    \n",
    "    # Prediction\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))  \n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#### Jist about dataset objects.  \n",
    "\n",
    "- These objects are sub-scriptable like any other data.\n",
    "- Every Index of the object is a tensor.\n",
    "- Every subscribable entry is one example. data[1] gives you the first example of the data\n",
    "\n",
    "More info - https://courses.edx.org/courses/course-v1:IBM+DL0110EN+3T2018/courseware/b35973888fed4695b23333400faf1d6c/51f5e23d6f6942dbbd1248ef49bbd4b7/6?activate_block_id=block-v1%3AIBM%2BDL0110EN%2B3T2018%2Btype%40vertical%2Bblock%40dd9ec4d1527a4ca29bf1aed73b82fe89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# Trainloader lets you extract the data in x and y\n",
    "\n",
    "def train(model, criterion, train_loader, validation_loader, optimizer, epochs=100):\n",
    "    i = 0\n",
    "    useful_stuff = {'training_loss': [], 'validation_accuracy': []}  \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            z = model(x.view(-1, 28 * 28))\n",
    "            loss = criterion(z, y)\n",
    "            loss.backward()\n",
    "            #this decreases the model parameters according to the values retreived in above function\n",
    "            optimizer.step()\n",
    "            useful_stuff['training_loss'].append(loss.data.item())\n",
    "        \n",
    "        correct = 0\n",
    "        for x, y in validation_loader:\n",
    "            yhat = model(x.view(-1, 28 * 28))\n",
    "            _, label = torch.max(yhat, 1)\n",
    "            correct += (label == y).sum().item()\n",
    "    \n",
    "        accuracy = 100 * (correct / len(validation_dataset))\n",
    "        useful_stuff['validation_accuracy'].append(accuracy)\n",
    "    \n",
    "    return useful_stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the validating dataset\n",
    "\n",
    "validation_dataset = dset.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters for create the model\n",
    "# image is of dimension 28*28. It feeds in the network. Output is of a digit from 1 to 10.\n",
    "\n",
    "input_dim = 28 * 28\n",
    "hidden_dim1 = 50\n",
    "hidden_dim2 = 50\n",
    "output_dim = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of iterations\n",
    "\n",
    "cust_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training data loader and validation data loader object\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=2000, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x11a1d40f0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Jist of optimisers](https://pytorch.org/docs/stable/optim.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with sigmoid function. Batch size is passed here only.\n",
    "\n",
    "learning_rate = 0.01\n",
    "model = Net(input_dim, hidden_dim1, hidden_dim2, output_dim)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "training_results = train(model, criterion, train_loader, validation_loader, optimizer, epochs=cust_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training_loss': [6.614238739013672,\n",
       "  6.60515022277832,\n",
       "  6.586878776550293,\n",
       "  6.580187797546387,\n",
       "  6.563568592071533,\n",
       "  6.547549247741699,\n",
       "  6.542593955993652,\n",
       "  6.5282368659973145,\n",
       "  6.503116607666016,\n",
       "  6.483527183532715,\n",
       "  6.4782280921936035,\n",
       "  6.468458652496338,\n",
       "  6.459653377532959,\n",
       "  6.445224761962891,\n",
       "  6.4205851554870605,\n",
       "  6.412880897521973,\n",
       "  6.39442253112793,\n",
       "  6.387928485870361,\n",
       "  6.379085540771484,\n",
       "  6.371450424194336,\n",
       "  6.3432745933532715,\n",
       "  6.325780391693115,\n",
       "  6.323033809661865,\n",
       "  6.2979631423950195,\n",
       "  6.284571170806885,\n",
       "  6.272216796875,\n",
       "  6.26621675491333,\n",
       "  6.244178771972656,\n",
       "  6.239214897155762,\n",
       "  6.2112250328063965,\n",
       "  6.199461460113525,\n",
       "  6.199078559875488,\n",
       "  6.182709693908691,\n",
       "  6.17387580871582,\n",
       "  6.1443634033203125,\n",
       "  6.144848823547363,\n",
       "  6.113442420959473,\n",
       "  6.109556198120117,\n",
       "  6.095207691192627,\n",
       "  6.075138568878174,\n",
       "  6.067172050476074,\n",
       "  6.049929141998291,\n",
       "  6.0437750816345215,\n",
       "  6.029205799102783,\n",
       "  6.005720615386963,\n",
       "  5.996801376342773,\n",
       "  5.98129940032959,\n",
       "  5.9709672927856445,\n",
       "  5.955851078033447,\n",
       "  5.931590557098389,\n",
       "  5.930161952972412,\n",
       "  5.9188432693481445,\n",
       "  5.911016464233398,\n",
       "  5.880084991455078,\n",
       "  5.879341125488281,\n",
       "  5.849335193634033,\n",
       "  5.8426032066345215,\n",
       "  5.8335747718811035,\n",
       "  5.824804782867432,\n",
       "  5.794398307800293,\n",
       "  5.787625789642334,\n",
       "  5.779711723327637,\n",
       "  5.768598556518555,\n",
       "  5.739981651306152,\n",
       "  5.728699684143066,\n",
       "  5.723412036895752,\n",
       "  5.702720642089844,\n",
       "  5.695621013641357,\n",
       "  5.670677661895752,\n",
       "  5.652584075927734,\n",
       "  5.648186683654785,\n",
       "  5.641599655151367,\n",
       "  5.619710445404053,\n",
       "  5.60065221786499,\n",
       "  5.5980072021484375,\n",
       "  5.573079586029053,\n",
       "  5.562171936035156,\n",
       "  5.549439907073975,\n",
       "  5.536543369293213,\n",
       "  5.524599552154541,\n",
       "  5.514669418334961,\n",
       "  5.493968963623047,\n",
       "  5.48024845123291,\n",
       "  5.454719543457031,\n",
       "  5.446562767028809,\n",
       "  5.4313578605651855,\n",
       "  5.426209926605225,\n",
       "  5.399270534515381,\n",
       "  5.390456199645996,\n",
       "  5.369103908538818,\n",
       "  5.367005348205566,\n",
       "  5.352924823760986,\n",
       "  5.329818248748779,\n",
       "  5.323786735534668,\n",
       "  5.311233997344971,\n",
       "  5.285041809082031,\n",
       "  5.279660224914551,\n",
       "  5.264842987060547,\n",
       "  5.259008884429932,\n",
       "  5.231456756591797,\n",
       "  5.20770788192749,\n",
       "  5.202672004699707,\n",
       "  5.190213203430176,\n",
       "  5.169805526733398,\n",
       "  5.148899555206299,\n",
       "  5.147299289703369,\n",
       "  5.136903285980225,\n",
       "  5.118895530700684,\n",
       "  5.084298610687256,\n",
       "  5.094886302947998,\n",
       "  5.079119682312012,\n",
       "  5.060981273651123,\n",
       "  5.036337852478027,\n",
       "  5.025765895843506,\n",
       "  5.008777141571045,\n",
       "  4.99992036819458,\n",
       "  4.979150772094727,\n",
       "  4.977447032928467,\n",
       "  4.966667175292969,\n",
       "  4.938750743865967,\n",
       "  4.934844493865967,\n",
       "  4.922525882720947,\n",
       "  4.90157413482666,\n",
       "  4.8766655921936035,\n",
       "  4.8625288009643555,\n",
       "  4.848557949066162,\n",
       "  4.843366622924805,\n",
       "  4.827263355255127,\n",
       "  4.811990737915039,\n",
       "  4.790560245513916,\n",
       "  4.785449981689453,\n",
       "  4.760034084320068,\n",
       "  4.740736484527588,\n",
       "  4.740501403808594,\n",
       "  4.721677780151367,\n",
       "  4.710946559906006,\n",
       "  4.687085151672363,\n",
       "  4.679908752441406,\n",
       "  4.665976047515869,\n",
       "  4.655973434448242,\n",
       "  4.627814292907715,\n",
       "  4.607851505279541,\n",
       "  4.581884384155273,\n",
       "  4.5926361083984375,\n",
       "  4.566921234130859,\n",
       "  4.5520830154418945,\n",
       "  4.545746326446533,\n",
       "  4.52604866027832,\n",
       "  4.512743949890137,\n",
       "  4.495482444763184,\n",
       "  4.4814839363098145,\n",
       "  4.469428062438965,\n",
       "  4.441697120666504,\n",
       "  4.451854705810547,\n",
       "  4.41935920715332,\n",
       "  4.40740442276001,\n",
       "  4.395744323730469,\n",
       "  4.3868408203125,\n",
       "  4.362481594085693,\n",
       "  4.348949432373047,\n",
       "  4.334083557128906,\n",
       "  4.314894676208496,\n",
       "  4.306551456451416,\n",
       "  4.296679496765137,\n",
       "  4.280035495758057,\n",
       "  4.276132106781006,\n",
       "  4.245224475860596,\n",
       "  4.228982925415039,\n",
       "  4.224912643432617,\n",
       "  4.216104507446289,\n",
       "  4.185726165771484,\n",
       "  4.192196369171143,\n",
       "  4.167386054992676,\n",
       "  4.1507248878479,\n",
       "  4.1370978355407715,\n",
       "  4.122915744781494,\n",
       "  4.114111423492432,\n",
       "  4.081222057342529,\n",
       "  4.0800628662109375,\n",
       "  4.0743584632873535,\n",
       "  4.053628444671631,\n",
       "  4.046694755554199,\n",
       "  4.026852607727051,\n",
       "  3.9989681243896484,\n",
       "  4.002115726470947,\n",
       "  3.981586456298828,\n",
       "  3.9673125743865967,\n",
       "  3.948451042175293,\n",
       "  3.930809497833252,\n",
       "  3.9257867336273193,\n",
       "  3.921274423599243,\n",
       "  3.8988451957702637,\n",
       "  3.8828301429748535,\n",
       "  3.8736822605133057,\n",
       "  3.857056140899658,\n",
       "  3.8469793796539307,\n",
       "  3.8320822715759277,\n",
       "  3.8229339122772217,\n",
       "  3.8023736476898193,\n",
       "  3.791019916534424,\n",
       "  3.7867259979248047,\n",
       "  3.7657957077026367,\n",
       "  3.748950242996216,\n",
       "  3.7420825958251953,\n",
       "  3.732853412628174,\n",
       "  3.7120137214660645,\n",
       "  3.7038586139678955,\n",
       "  3.6893131732940674,\n",
       "  3.6811063289642334,\n",
       "  3.6591103076934814,\n",
       "  3.656674385070801,\n",
       "  3.630721092224121,\n",
       "  3.6272714138031006,\n",
       "  3.6166608333587646,\n",
       "  3.599747896194458,\n",
       "  3.5797252655029297,\n",
       "  3.5795488357543945,\n",
       "  3.5759568214416504,\n",
       "  3.5511789321899414,\n",
       "  3.539931297302246,\n",
       "  3.5350375175476074,\n",
       "  3.5221545696258545,\n",
       "  3.5023038387298584,\n",
       "  3.496854305267334,\n",
       "  3.4844911098480225,\n",
       "  3.4807252883911133,\n",
       "  3.4595820903778076,\n",
       "  3.4573583602905273,\n",
       "  3.4420101642608643,\n",
       "  3.426867961883545,\n",
       "  3.4190616607666016,\n",
       "  3.411243200302124,\n",
       "  3.4036097526550293,\n",
       "  3.389538526535034,\n",
       "  3.3821725845336914,\n",
       "  3.3695788383483887,\n",
       "  3.3488681316375732,\n",
       "  3.3459198474884033,\n",
       "  3.3331053256988525,\n",
       "  3.327820062637329,\n",
       "  3.322934865951538,\n",
       "  3.3040401935577393,\n",
       "  3.2898945808410645,\n",
       "  3.287656784057617,\n",
       "  3.267812490463257,\n",
       "  3.2613916397094727,\n",
       "  3.2525835037231445,\n",
       "  3.246490955352783,\n",
       "  3.23626971244812,\n",
       "  3.235513687133789,\n",
       "  3.2141926288604736,\n",
       "  3.2196059226989746,\n",
       "  3.193810224533081,\n",
       "  3.188232183456421,\n",
       "  3.1878697872161865,\n",
       "  3.171142101287842,\n",
       "  3.165220022201538,\n",
       "  3.1544153690338135,\n",
       "  3.1516830921173096,\n",
       "  3.142435312271118,\n",
       "  3.1351256370544434,\n",
       "  3.114589214324951,\n",
       "  3.1181869506835938,\n",
       "  3.1073451042175293,\n",
       "  3.0935707092285156,\n",
       "  3.09330153465271,\n",
       "  3.077679395675659,\n",
       "  3.0744986534118652,\n",
       "  3.0671908855438232,\n",
       "  3.0579299926757812,\n",
       "  3.050009250640869,\n",
       "  3.0413119792938232,\n",
       "  3.031013011932373,\n",
       "  3.0264503955841064,\n",
       "  3.0272727012634277,\n",
       "  3.0132546424865723,\n",
       "  3.008298635482788,\n",
       "  2.999234914779663,\n",
       "  2.9905710220336914,\n",
       "  2.9895217418670654,\n",
       "  2.9856035709381104,\n",
       "  2.9708259105682373,\n",
       "  2.9635815620422363,\n",
       "  2.9731264114379883,\n",
       "  2.953742265701294,\n",
       "  2.9441564083099365,\n",
       "  2.9382872581481934,\n",
       "  2.9311776161193848,\n",
       "  2.9276795387268066,\n",
       "  2.923312187194824,\n",
       "  2.9202449321746826,\n",
       "  2.91434383392334,\n",
       "  2.905108690261841,\n",
       "  2.9006612300872803,\n",
       "  2.8967463970184326,\n",
       "  2.889693260192871,\n",
       "  2.8861474990844727,\n",
       "  2.8768179416656494,\n",
       "  2.8704400062561035,\n",
       "  2.864926338195801],\n",
       " 'validation_accuracy': [8.92,\n",
       "  10.32,\n",
       "  10.32,\n",
       "  10.32,\n",
       "  10.32,\n",
       "  10.32,\n",
       "  10.32,\n",
       "  11.35,\n",
       "  11.35,\n",
       "  11.35]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuition behind everything dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_dataset[3000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this returns x value of tensor\n",
    "type(train_dataset[3000][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this returns y value\n",
    "train_dataset[3000][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout\n",
    "- Regularisation technique for nueral networks  \n",
    "- In pytorch more probablity means more nuerons per layer. Dropout is used to lessen this probablity. In other words some nodes randomly as not choosen so overfitting does not occur.\n",
    "- Some nodes are turn to zero. Some doesn't. Which is determined by a probablity value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self,in_size,n_hidden,out_size,p=0):\n",
    "        super(Net,self).__init__()\n",
    "        self.drop=nn.Dropout(p=p)\n",
    "        self.linear1=nn.Linear(in_size,n_hidden)\n",
    "        self.linear2=nn.Linear(n_hidden,n_hidden)\n",
    "        self.linear3=nn.Linear(n_hidden,out_size)\n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.linear1(x))\n",
    "        x=self.dropout(x)\n",
    "        x=F.relu(self.linear2(x))\n",
    "        x=self.drop(x)\n",
    "        x=self.linear3(x)\n",
    "        return x\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MAIN",
   "language": "python",
   "name": "main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
